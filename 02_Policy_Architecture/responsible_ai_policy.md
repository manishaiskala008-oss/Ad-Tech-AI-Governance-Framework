Responsible AI & Ad-Tech Safety Policy (v1.0)
Scope: All generative AI models and automated targeting algorithms used for ad creation and delivery. Regulatory Alignment: NIST AI RMF, Indian DPDP Act (2023), and ASCI Guidelines.

1. Core Governance Principles
Safety by Design: AI models must undergo "Red Teaming" for toxicity and brand safety prior to any live campaign deployment.

Creative Parity: Algorithms must be audited to ensure that "High-Value" ad variants are not disproportionately withheld from protected demographic groups (Gender, Age, Religion).

Consumer Transparency: In accordance with ASCI mandates, all AI-generated influencers or photorealistic synthetic imagery must carry a visible "AI-Generated" disclosure.

2. Technical Guardrails & Controls
PII Sanitization: All training data and prompt inputs must be scrubbed of Personally Identifiable Information (PII) to comply with the DPDP Act.

Human-in-the-Loop (HITL) Triggers: * Level 1 (Low Risk): Fully automated generation (e.g., color testing).

Level 2 (Medium Risk): Mandatory manual review for "Sensitive Verticals" (Healthcare, Finance).

Level 3 (High Risk): Executive sign-off for models targeting minors or vulnerable populations.

Explainability (XAI) Mandate: We will use SHAP or LIME values to document the "Decision Logic" of targeting models to provide an audit trail for regulatory inquiries.

3. Monitoring & Enforcement
Drift Detection: Monthly "Model Health Checks" to identify if targeting logic has drifted into biased behavior due to feedback loops.

Non-Compliance Protocol: Immediate "Kill Switch" activation for any model found to be generating non-compliant content, followed by a formal Root Cause Analysis (RCA).
